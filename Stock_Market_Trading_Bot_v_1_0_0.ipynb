{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Market Trading Bot - v.1.0.0",
      "provenance": [],
      "authorship_tag": "ABX9TyMfexMXrSyDjQkWF9Z+9RDW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leopedroso1/StockMarket_Bot/blob/main/Stock_Market_Trading_Bot_v_1_0_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZm0X2zP5TRq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import itertools\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_aPxMbiCkmW"
      },
      "source": [
        "# Gathering Data\n",
        "\n",
        "def get_data():\n",
        "  # Returns a T x 3 list of stock prices\n",
        "  # Each row contains a different stock\n",
        "  # 0 - AAPL\n",
        "  # 1 - MST\n",
        "  # 2 - SBUX\n",
        "\n",
        "  df = pd.read_csv(\"aapl_msi_sbux.csv\")\n",
        "\n",
        "  return df.values"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9rDWtEVC1mS"
      },
      "source": [
        "# Replay Buffer\n",
        "# This is the experience replay in the memory\n",
        "class ReplayBuffer:\n",
        "\n",
        "  # Initialize our Buffer\n",
        "  # ptr stands for pointer\n",
        "  def __init__(self, obs_dim, act_dim, size):\n",
        "\n",
        "    self.obs1_buf= np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.obs2_buf= np.zeros([size, obs_dim], dtype=np.float32)\n",
        "    self.acts_buf= np.zeros(size, dtype= np.unit8)\n",
        "    self.rews_buf= np.zeros(size, dtype= np.float32)\n",
        "    self.done_buf= np.zeros(size, dtype= np.unit8)\n",
        "    self.ptr, self.size, self.max_size= 0, 0, size\n",
        "\n",
        "  # Store elemens in the buffer as a carousel\n",
        "  def store(self, obs, act, rew, next_obs, done):\n",
        "\n",
        "    self.obs1_buf[self.ptr]= obs\n",
        "    self.obs2_buf[self.ptr]= next_obs\n",
        "    self.acts_buf[self.ptr]= act\n",
        "    self.rews_buf[self.ptr]= rew\n",
        "    self.done_buf[self.ptr]= done\n",
        "    self.ptr= (self.ptr + 1) % self.max_size\n",
        "    self.size= min(self.size + 1, self.max_size)\n",
        "\n",
        "  # Generate a random batch from 0 to the size of the buffer\n",
        "  def sample_batch(self, batch_size= 32):\n",
        "\n",
        "    idxs = np.random.randint(0, self.size, size= batch_size)\n",
        "    return dict(s= self.obs1_buf[idxs],\n",
        "                s2= self.obs2_buf[idxs],\n",
        "                a= self.acts_buf[idxs],\n",
        "                r= self.rews_buf[idxs],\n",
        "                d= self.done_buf[idxs])\n",
        "\n",
        "  # Return a scikit-learn scaler object to scale the states\n",
        "  # Note: You could also populate the replay buffer here\n",
        "  # Run for multiple episodes in order to get accuracy\n",
        "  def get_scaler(env):\n",
        "\n",
        "    states = []\n",
        "\n",
        "    for _ in range(env.n_step):\n",
        "\n",
        "      action= np.random.choice(env.action_space)\n",
        "      state, reward, done, info = env.step(action)\n",
        "      states.append(state)\n",
        "\n",
        "      if done:\n",
        "\n",
        "        break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "\n",
        "    return scaler\n",
        "  \n",
        "  def maybe_make_dir(directory):\n",
        "\n",
        "    if not os.path.exists(directory):\n",
        "\n",
        "      os.makedirs(directory)\n",
        "\n",
        "  # Create our model ==> Multi Layer Perceptron\n",
        "  def mlp(input_dim, n_action, n_hidden_layers= 1, hidden_dim= 32):\n",
        "\n",
        "    # Input Layer\n",
        "    i = Input(shape= (input_dim,))\n",
        "    x = i\n",
        "\n",
        "    # Hidden Layers\n",
        "    for _ in range(n_hidden_layers):\n",
        "\n",
        "      x = Dense(hidden_dim, activation='relu')(x)\n",
        "\n",
        "    # Output Layer\n",
        "    x = Dense(n_action)(x)\n",
        "\n",
        "    # Build the model\n",
        "    model = Model(i, x)\n",
        "    model.compile(loss= 'mse', optimizer= 'adam')\n",
        "    print(model.summary())\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BKxdxKYbqYC"
      },
      "source": [
        "class MultiStockEnv:\n",
        "\n",
        "  \"\"\"\n",
        "  A 3-Stock trading environment \n",
        "\n",
        "  State: Vector of size 7 (n_stock * 2 + 1)\n",
        "    - # shares of stock 1 owned\n",
        "    - # shares of stock 2 owned\n",
        "    - # shares of stock 3 owned\n",
        "    - Price of stock 1 (using only close price)\n",
        "    - Price of stock 2 (using only close price)\n",
        "    - Price of stock 3 (using only close price)\n",
        "    - cash owned (Can be used to purchase more stocks)\n",
        "  \n",
        "  Action: categorical variable with 27 (3^3) possibilities\n",
        "    - for each stock, you can:\n",
        "    - 0 = sell\n",
        "    - 1 = hold\n",
        "    - 2 = buy\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data, initial_investment= 20000):\n",
        "\n",
        "    # Loading data\n",
        "    self.stock_price_history= data\n",
        "    self.n_step, self.n_stock= self.stock_price_history.shape\n",
        "\n",
        "    # Instance attributes\n",
        "    self.initial_investment= initial_investment\n",
        "    self.cur_step= None\n",
        "    self.stock_owned= None\n",
        "    self.stock_price= None\n",
        "    self.cash_in_hand= None\n",
        "\n",
        "    self.action_space= np.arrange(3 ** self.n_stock)\n",
        "\n",
        "    # Action Permutations\n",
        "    # Returns a nested list with elements like:\n",
        "    # [0,0,0] >> Sell all stocks\n",
        "    # [0,0,1] >> Sell 1st and 2nd stock but hold 3rd\n",
        "    # [0,0,2]\n",
        "    # [0,1,0]\n",
        "    # [0,1,1]...\n",
        "    # Where:\n",
        "    # 0 = sell\n",
        "    # 1 = hold\n",
        "    # 2 = buy\n",
        "    self.action_list= list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
        "\n",
        "    # Calculate the size of state\n",
        "    self.sate_dim= self.n_stock * 2 + 1\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "  def reset():\n",
        "\n",
        "    self.cur_step= 0\n",
        "    self.stock_owned= np.zeros(self.n_stock)\n",
        "    self.stock_price= self.stock_price_history[self.cur_step]\n",
        "    self.cash_in_hand= self.initial_investment\n",
        "    \n",
        "    return self._get_obs()\n",
        "  \n",
        "  def step(self, action):\n",
        "\n",
        "    assert action in self.action_space\n",
        "\n",
        "    # Get current value before performing the action\n",
        "    prev_val= self._get_val()\n",
        "\n",
        "    # Update price, i.e. go to the next day\n",
        "    self.cur_step += 1\n",
        "    self.stock_price= self.stock_price_history[self.cur_step]\n",
        "\n",
        "    # Perform trade\n",
        "    self._trade(action)\n",
        "\n",
        "    # Get the new value after taking the action\n",
        "    cur_val= self._get_val()\n",
        "\n",
        "    # Reward is the increase in Portifolio value\n",
        "    reward= cur_val - prev_val\n",
        "\n",
        "    # Done if we have run out of data\n",
        "    done= self.cur_step == self.n_step - 1\n",
        "\n",
        "    # Store the current value of the portifolio here\n",
        "    info = {'cur_val': cur_val}\n",
        "\n",
        "    # Return the value similarly to Gym API\n",
        "    return self._get_obs(), reward, done, info\n",
        "\n",
        "  def _get_obs(self):\n",
        "\n",
        "    obs= np.empty(self.state_dim)\n",
        "    obs[:self.n_stock]= self.stock_owned\n",
        "    obs[self.n_stock: 2 * self.n_stock]= self.stock_price\n",
        "    obs[-1]= self.cash_in_hand\n",
        "\n",
        "    return obs\n",
        "\n",
        "  def _get_val(self):\n",
        "\n",
        "    return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "  def _trade(self, action):\n",
        "\n",
        "    # Recall: Index the action to be performed\n",
        "    # 0 --> Sell\n",
        "    # 1 --> Hold\n",
        "    # 2 --> Buy\n",
        "\n",
        "    # e.g\n",
        "    # [0, 2, 1] Stock 1 >> Sell / Stock 2 >> Buy / Stock 3 >> Hold\n",
        "\n",
        "    action_vec= self.action_list[action]\n",
        "\n",
        "    # Determine which stocks to buy or sell\n",
        "    sell_index= [] # >> Stores index to sell\n",
        "    buy_index= [] # >> Stores index to buy\n",
        "\n",
        "    for i, a in enumerate(action_vec):\n",
        "\n",
        "      if a == 0:\n",
        "        sell_index.append(i)\n",
        "      \n",
        "      elif a== 2:\n",
        "        buy_index.append(i)\n",
        "\n",
        "    # Sell any stocks we want to sell then buy any stocks we want to buy\n",
        "    if sell_index:\n",
        "\n",
        "      # NOTE: To simplify the problem, when we sell, we will sell ALL shares of that stock\n",
        "      for i in sell_index:\n",
        "        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
        "        self.stock_owned[i] = 0\n",
        "      \n",
        "    if buy_index:\n",
        "\n",
        "      # NOTE: When buying, we will loop through each stock we want to buy and buy one share at a time until we run out of cash. Buy as much as we can!\n",
        "      for i in buy_index:\n",
        "\n",
        "        can_buy= True\n",
        "        while can_buy:\n",
        "          for i in buy_index:\n",
        "\n",
        "            if self.cash_in_hand > self.stock_price[i]:\n",
        "              self.stock_owned[i] += 1\n",
        "              self.cash_in_hand -= self.stock_price[i]\n",
        "\n",
        "            else:\n",
        "              can_buy= False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDvd9ueBpe8B"
      },
      "source": [
        "class DQNAgent(object):\n",
        "\n",
        "  def __init__(self, state_size, action_size):\n",
        "\n",
        "    self.state_size= state_size\n",
        "    self.action_size= action_size\n",
        "    self.memory= ReplayBuffer(state_size, action_size, size= 500)\n",
        "    self.gamma= 0.95 # Discount Rate\n",
        "    self.epsilon= 1.0 # Exploration Rate\n",
        "    self.epsilon_min= 0.01\n",
        "    self.epsilon_decay= 0.995\n",
        "    self.model= mlp(state_size, action_size)\n",
        "\n",
        "    def update_replay_memory(self, state, action, reward, next_state, done):\n",
        "      self.memory.store(state, action, reward, next_state, done)\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "      if np.random.rand() <= self.epsilon:\n",
        "\n",
        "        return np.random.choice(self.action_size)\n",
        "      \n",
        "      act_values= self.model.predict(state)\n",
        "      \n",
        "      return np.argmax(act_values[0]) # Returns the action\n",
        "      \n",
        "    def replay(self, batch_size= 32):\n",
        "\n",
        "      # First check if replay buffer contains enough data\n",
        "      if self.memory.size < batch_size:\n",
        "        return\n",
        "      \n",
        "      # Sample a batch of data from the replay memory\n",
        "      minibatch= self.memory.sample_batch(batch_size)\n",
        "      states= minibatch['s']\n",
        "      actions= minibatch['a']\n",
        "      rewards= minibatch['r']\n",
        "      next_states= minibatch['s2']\n",
        "      done= minibatch['d']\n",
        "\n",
        "      # Calculate the tentative target Q(s', a)\n",
        "      target= rewards + self.gama * np.amax(self.model.precit(next_states), axis=1)\n",
        "\n",
        "      # The value of terminal states is Zero, so set the target to be the reward only\n",
        "      target[done]= rewards[done]\n",
        "\n",
        "      # With Keras API, the target (usually) must have the same shape of its predictions\n",
        "      # However, we only need to update the network for the actions which were actually taken\n",
        "      # We can accomplish this by setting the target to be equal to the prediction for all values\n",
        "      # Then, only charge the targets for the actions taken Q(s, a)\n",
        "      target_full= self.model.predict(states)\n",
        "      target_full[np.arrange(batch_size), actions]= target\n",
        "\n",
        "      # Run on training step\n",
        "      self.model.train_on_batch(states, target_full)\n",
        "\n",
        "      if self.epsilon > self.epsilon_min:\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "      \n",
        "    def load(self, name):\n",
        "      self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "      self.model.save_weights(name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoYaymtYe09_"
      },
      "source": [
        "def play_one_episode(agent, env, is_train):\n",
        "  # NOTE: After transforming, states are already 1 x D\n",
        "  state= env.reset()\n",
        "  state= scaler.transform([state])\n",
        "  done= False\n",
        "\n",
        "  while not done:\n",
        "    action= agent.act(state)\n",
        "    next_state, reward, done, info= env.step(action)\n",
        "    next_state= scaler.transform([next_state])\n",
        "\n",
        "    if is_train == 'train':\n",
        "    \n",
        "      agent.update_replay_memory(state, action, reward, next_state, done)\n",
        "      agent.replay(batch_size)\n",
        "\n",
        "    state = next_state\n",
        "\n",
        "  return info['cur_val'] # Current value of our portifolio\n",
        "\n",
        "if __main__ = '__main__':\n",
        "\n",
        "  # Config setup\n",
        "  models_folder= 'rl_trader_models'\n",
        "  rewards_folder= 'rl_trader_rewards'\n",
        "  num_episodes= 2000\n",
        "  batch_size= 32\n",
        "  initial_investment= 20000\n",
        "\n",
        "  # Allows us run this code with command line arguments\n",
        "  parser= argparse.ArgumentParser()\n",
        "  parser.add_argument('-m', '--mode', type=str, required= True, help='either \"train\" or \"test\"')\n",
        "\n",
        "  args= parser.parse_args()\n",
        "\n",
        "  maybe_make_dir(models_folder)\n",
        "  maybe_make_dir(rewards_folder)\n",
        "\n",
        "  data= get_data()\n",
        "  n_timesteps, n_stocks = data.shape\n",
        "\n",
        "  n_train = n_timesteps // 2 \n",
        "\n",
        "  train_data = data[:n_train]\n",
        "  test_data = data[n_train:]\n",
        "\n",
        "  env= MultiStockEnv(train_data, initial_investment)\n",
        "  state_size= env.state_dim\n",
        "  action_size= len(env.action_space)\n",
        "  agent= DQNAgent(state_size, action_size)\n",
        "  scaler= get_scaler(env)\n",
        "\n",
        "  # Store the final value of the portifolio (End of episode)\n",
        "  portifolio_value= []\n",
        "\n",
        "  if args.mode == \"test\":\n",
        "\n",
        "    # Then load the previous scaler\n",
        "    with open(f'{models_folder}/scaler.pkl','rb') as f:\n",
        "      scaler= picke.load()\n",
        "\n",
        "    # Remake the env with test data\n",
        "    env= MultiStockEnv(test_data, initial_investment)\n",
        "\n",
        "    # Make sure epsilon is not 1\n",
        "    # No need to run multiple episodes if epsilon= 0, it is deterministic\n",
        "    agent.epsilon= 0.01\n",
        "\n",
        "    # Load trained weights\n",
        "    agent.load(f'{models_folder}/dqn.h5')\n",
        "\n",
        "  for e in range(num_episodes):\n",
        "    t0= datetime.now()\n",
        "    val= play_one_episode(agent, env, args.mode)\n",
        "    dt= datetime.now() - t0\n",
        "    print(f\"episode: {e + 1} / {num_episodes}, episode end value: {val:.2f}, duration: {dt}\")\n",
        "    portifolio_value.append(val) # Append episode end portifolio value\n",
        "\n",
        "  # Save the weights we are done \n",
        "  if agrs.mode == \"train\":\n",
        "\n",
        "    # Save DQN\n",
        "    agent.save(f'{models_folder}/dqn.h5')\n",
        "\n",
        "    # Save Scaler\n",
        "    with open(f'{models_folder}/scaler.pkl','wb') as f:\n",
        "      pickle.dump(scaler, f)\n",
        "  \n",
        "  np.save(f'{rewards_folder}/{args.mode}.npy', portfolio_value)\n",
        "\n",
        "\n",
        "\n",
        "# TO EXECUTE: python rl_trader.py -m train && python plot_rl_rewards.py -m train\n",
        "# TO EXECUTE: python rl_trader.py -m test && python plot_rl_rewards.py -m test\n",
        "# TEST WITH OTHER STOCKS THAT DECREASES THE VALUE AND EXPLORE NEW WAYS "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-fnzQEMzjkd"
      },
      "source": [
        "# Plotting the rewards that we saved\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "\n",
        "parser= argparse.ArgumentParser()\n",
        "parser.add_argument('-m', '--mode', type=str, required= True, help='either \"train\" or \"test\" ')\n",
        "\n",
        "args= parser.parse_args()\n",
        "\n",
        "a = np.load(f'linear_rl_trader_rewards/{args.mode}.npy')\n",
        "\n",
        "print(f\"average reward: {a.mean().2f}, min: {a.min():.2f}, max:{a.max():.2f}\")\n",
        "\n",
        "plt.hist(a, bins=20)\n",
        "plt.title(args.mode)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}